{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1lLWMYUfJ5Ir79nHC2mP7Dck_Qt5Q8plE",
      "authorship_tag": "ABX9TyPPrTtz72rIm3sUZIuygrVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastiantonn/phd/blob/main/chapter4/unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors: Sebastian Tonn and Mon-Ray Shao\n",
        "# Translational Plant Biology Group, Utrecht University\n",
        "\n",
        "# Script to train, test and deploy a convolutional neural network based on the U-NET architecture for\n",
        "# semantic segmentation of RGB UV-fluorescence images of lettuce leaves into background, red fluorescent (not infected)\n",
        "# and blue-green fluorescent (downy mildew infected) pixels."
      ],
      "metadata": {
        "id": "yFoJNnWlNtax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test github"
      ],
      "metadata": {
        "id": "h8FKYjxHgcfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiyTJDZufC3F"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal-loss\n",
        "from focal_loss import BinaryFocalLoss\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "\n"
      ],
      "metadata": {
        "id": "xyoPjX_CfuYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to working directory and folders within that directory that contain downsampled images and downsampled labels\n",
        "wdir = '/content/drive/MyDrive/PhD/UV-BGF/230627_unet-test_individual-leaves/'\n",
        "input_img_dir = '/content/drive/MyDrive/PhD/UV-BGF/230627_unet-test_individual-leaves/singleleaf_images_allbatches_downsampled'\n",
        "label_dir = '/content/drive/MyDrive/PhD/UV-BGF/230627_unet-test_individual-leaves/singleleaf_3labels_allbatches_downsampled'\n",
        "\n",
        "# Set image\n",
        "input_img_paths = sorted([os.path.join(input_img_dir, fname)\n",
        "                          for fname in os.listdir(input_img_dir)\n",
        "                          if fname.endswith(\".png\")and not fname.startswith(\".\")])\n",
        "\n",
        "label_paths = sorted([os.path.join(label_dir, fname)\n",
        "                       for fname in os.listdir(label_dir)\n",
        "                       if fname.endswith(\".png\") and not fname.startswith(\".\")])\n"
      ],
      "metadata": {
        "id": "t_DNE5vhG2Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set input parameters\n",
        "img_size = (480, 480) # (height, width)\n",
        "num_imgs = len(input_img_paths)"
      ],
      "metadata": {
        "id": "tnEihFnmMwVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load images and masks and convert them to arrays. Also add another 4th axis to mask arrays.\n",
        "input_imgs = [cv2.imread(img, 1) for img in input_img_paths]\n",
        "input_imgs = np.array(input_imgs)\n",
        "labels = [cv2.imread(mask, 0) for mask in label_paths]\n",
        "labels = np.array(labels)\n",
        "labels = np.expand_dims(labels, axis = 3)"
      ],
      "metadata": {
        "id": "odqIyBcqSNWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimensions of image and label arrays\n",
        "print('Image array shape:', input_imgs.shape, '; Label array shape:', labels.shape)"
      ],
      "metadata": {
        "id": "PKQZNr4EUqXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize image data set so values are between 0 and 1\n",
        "input_imgs = input_imgs / 255"
      ],
      "metadata": {
        "id": "wF8072j-gVcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimensions of image and label arrays\n",
        "print('Image array shape:', input_imgs.shape, '; Label array shape:', labels.shape)\n",
        "print('Image array max value:', input_imgs.max(), '; Label array max value:', labels.max())"
      ],
      "metadata": {
        "id": "YKiEccj7g30H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed and testsize to split data into train, test and validation data data\n",
        "testsize = 0.2\n",
        "seed = 42\n",
        "# Split into train and test set\n",
        "train_input_imgs, test_input_imgs, train_labels, test_labels = train_test_split(input_imgs, labels, test_size = testsize, random_state = seed)\n",
        "# Split train into train and validation set\n",
        "train_input_imgs, val_input_imgs, train_labels, val_labels = train_test_split(train_input_imgs, train_labels, test_size = testsize, random_state = seed)"
      ],
      "metadata": {
        "id": "bATC1BwMNkp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dimensions of image and label arrays\n",
        "print('Training image array shape:', train_input_imgs.shape, '; Training label array shape:', train_labels.shape)\n",
        "print('Test image array shape:', test_input_imgs.shape, '; Test label array shape:', test_labels.shape)\n",
        "print('Validation image array shape:', val_input_imgs.shape, '; Validation label array shape:', val_labels.shape)"
      ],
      "metadata": {
        "id": "ckWFoaSTQBAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform labels into one-hot encoding\n",
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels, num_classes=3)\n",
        "val_labels = to_categorical(val_labels, num_classes=3)\n",
        "test_labels = to_categorical(test_labels, num_classes=3)"
      ],
      "metadata": {
        "id": "vKnCA6G4uE0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if images and labels match. To display the normalized (divided by 255) images containing values between 0 and 1 correctly\n",
        "# we have to set min and max values and interpolation in the imshow() function. ::-1 reverses the order of the color channels from BGR (cv2 format) to RGB\n",
        "img_number = random.randint(0, len(train_input_imgs)-1)\n",
        "plt.figure(figsize = (12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(train_input_imgs[img_number,:,:,::-1], vmin = 0, vmax = 2, interpolation='nearest')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(train_labels[img_number,:,:,:], cmap = 'gray')"
      ],
      "metadata": {
        "id": "c27ft_3hPO6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STAGE 2: TRAIN U-NET MODEL\n",
        "\n",
        "# define the CNN architecture\n",
        "num_classes = 3\n",
        "inputs = tf.keras.layers.Input((img_size[1], img_size[1], 3))\n",
        "\n",
        "# Contraction path\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "b1 = tf.keras.layers.BatchNormalization()(c1)\n",
        "r1 = tf.keras.layers.ReLU()(b1)\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(r1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "b2 = tf.keras.layers.BatchNormalization()(c2)\n",
        "r2 = tf.keras.layers.ReLU()(b2)\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(r2)\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "b3 = tf.keras.layers.BatchNormalization()(c3)\n",
        "r3 = tf.keras.layers.ReLU()(b3)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(r3)\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "b4 = tf.keras.layers.BatchNormalization()(c4)\n",
        "r4 = tf.keras.layers.ReLU()(b4)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(r4)\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "b5 = tf.keras.layers.BatchNormalization()(c5)\n",
        "r5 = tf.keras.layers.ReLU()(b5)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(r5)\n",
        "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "# expansive path\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "u6 = tf.keras.layers.BatchNormalization()(u6)\n",
        "u6 = tf.keras.layers.ReLU()(u6)\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "u7 = tf.keras.layers.BatchNormalization()(u7)\n",
        "u7 = tf.keras.layers.ReLU()(u7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(u7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "u8 = tf.keras.layers.BatchNormalization()(u8)\n",
        "u8 = tf.keras.layers.ReLU()(u8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(u8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "u9 = tf.keras.layers.BatchNormalization()(u9)\n",
        "u9 = tf.keras.layers.ReLU()(u9)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation='softmax')(u9) # sigmoid for binary, else softmax\n"
      ],
      "metadata": {
        "id": "QcKevNTHmFKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "# Focal loss: loss=BinaryFocalLoss(gamma=2), Binary crossentropy: loss='binary_crossentropy'\n",
        "# Optimizer = Adam(learning_rate = 1e-3) - tried between 1e-3 and 1e-4.\n",
        "# 6e-4, Focal2, batch 4, epoch 40 = 0.83\n",
        "# 6e-4, Focal2, batch 4, epoch 50 = 0.82\n",
        "# 6e-4, Focal1.5, batch 4, epoch 40 = 0.80\n",
        "# 6e-4, binary, batch 4, epoch 40 = 0.80\n",
        "\n",
        "# Binary classification\n",
        "#model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "#model.compile(optimizer=Adam(learning_rate = 1e-3), loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[1], threshold=0.5)]) #[tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)]\n",
        "#model.summary()\n",
        "\n",
        "# Three classes classification\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer=Adam(learning_rate = 8e-4),\n",
        "              loss=tf.keras.losses.CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0), #tf.keras.losses.CategoricalFocalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=3)])#[tf.keras.metrics.IoU(num_classes = 3, target_class_ids=[2])]) #[tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)]\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "c-3CKHMVn2Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer and loss, compile model; best epoch will be saved\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(wdir, 'bremia_segmentation.hdf5'), save_best_only = True)]\n",
        "\n",
        "# train according to parameters indicated\n",
        "history = model.fit(train_input_imgs, train_labels,\n",
        "                    epochs = 80, callbacks = callbacks, batch_size =7,\n",
        "                    validation_data = (val_input_imgs, val_labels))"
      ],
      "metadata": {
        "id": "XhWofXM0un0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss over epochs\n",
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label = \"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "# plot iou over epochs\n",
        "epochs = range(1, len(history.history[\"one_hot_mean_io_u\"]) + 1)\n",
        "binary_io_u = history.history[\"one_hot_mean_io_u\"]\n",
        "val_binary_io_u = history.history[\"val_one_hot_mean_io_u\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, binary_io_u, \"bo\", label = \"Training IOU\")\n",
        "plt.plot(epochs, val_binary_io_u, \"b\", label = \"Validation IOU\")\n",
        "plt.title(\"Training and validation IOU\")\n",
        "plt.legend()\n",
        "\n",
        "# plot accuracy over epochs\n",
        "# epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
        "# accuracy = history.history[\"accuracy\"]\n",
        "# val_accuracy = history.history[\"val_accuracy\"]\n",
        "# plt.figure()\n",
        "# plt.plot(epochs, accuracy, \"bo\", label = \"Training Accuracy\")\n",
        "# plt.plot(epochs, val_accuracy, \"b\", label = \"Validation Accuracy\")\n",
        "# plt.title(\"Training and validation Accuracy\")\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "1C1Qb2itn5M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load best model from history\n",
        "model = tf.keras.models.load_model(os.path.join(wdir, 'bremia_segmentation.hdf5'))"
      ],
      "metadata": {
        "id": "jsLDlYfb2wgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelname = '230918_model_val-train-test_batchsize7_LR-8e-4_70epochs.h5'"
      ],
      "metadata": {
        "id": "Qn2KhXQ5PyAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save this model to file\n",
        "model.save(os.path.join(wdir, modelname), save_format = \"h5\")"
      ],
      "metadata": {
        "id": "C4UvC9ULJeO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained U-Net model from file\n",
        "model = tf.keras.models.load_model(os.path.join(wdir, modelname))"
      ],
      "metadata": {
        "id": "SYMyCregC7va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check predictions for a random image out of the validation set\n",
        "\n",
        "# Generate random number\n",
        "img_number = random.randint(0, len(test_input_imgs)-1)\n",
        "# Load validation image with random number\n",
        "test_image = test_input_imgs[img_number]\n",
        "# Load corresponding ground truth (label) with random number\n",
        "ground_truth = test_labels[img_number]\n",
        "# Add axis 0 to image and check shape (this needs to be done because model.predict() expects the format with 4 axis)\n",
        "test_image_expanded = np.expand_dims(test_image, 0)\n",
        "print('Shape of expanded test image:', test_image_expanded.shape)\n",
        "# Predict image\n",
        "prediction = model.predict(test_image_expanded)\n",
        "# Check shape of prediction (this is now one hot encoded with the probability for each class on a separate 'channel' on the 3rd axis)\n",
        "print('Shape of prediction:', prediction.shape)\n",
        "# Revert one-hot encoding. Pixels get a value of 0, 1 or 2 (in case of three classes), depending on the highest probability on the 3rd axis.\n",
        "# We also drop the first axis by adding [0,:,:], resulting in a 480 x 480 array, i.e. gray scale image with values 0, 1, or 2.\n",
        "prediction = np.argmax(prediction, axis=3)[0,:,:]\n",
        "# Check shape of prediction after argmax\n",
        "print('Shape of prediction after argmax', prediction.shape)\n",
        "# Check values in prediction\n",
        "print('Unique values in prediction image:', np.unique(prediction))\n",
        "\n",
        "# Create plots to show original image, ground truth and prediction side by side\n",
        "plt.close(1)\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(2,3,1)\n",
        "plt.title('Test Image')\n",
        "plt.imshow(test_image[:,:,::-1], vmin = 0, vmax = 1, interpolation='nearest')\n",
        "plt.subplot(2,3,2)\n",
        "plt.title('Ground Truth')\n",
        "# Here we also revert the one-hot encoding of the ground truth to get a gray scale image, so we can use the same cmap as for the prediction.\n",
        "plt.imshow(np.argmax(ground_truth, axis = 2), vmin = 0, vmax = 2, cmap = 'viridis') # Define min and max so the color mapping is the same also if there are only pixel values 0 and 1\n",
        "plt.subplot(2,3,3)\n",
        "plt.title('Prediction')\n",
        "plt.imshow(prediction, vmin = 0, vmax = 2, cmap = 'viridis' )\n"
      ],
      "metadata": {
        "id": "Z51y5j2k4ctH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict validation set and revert one-hot encoding\n",
        "pred = model.predict(test_input_imgs)\n",
        "pred_argmax = np.argmax(pred, axis=3)\n",
        "\n",
        "# Revert one-hot encoding of validation labels\n",
        "test_labels_argmax = np.argmax(test_labels, axis = 3)\n"
      ],
      "metadata": {
        "id": "136ZFB9r_KBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Intersection-Over-Union (IOU) for validation set\n",
        "# iou = true_positives / (true_positives + false_positives + false_negatives)\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "iou = MeanIoU(num_classes=3)\n",
        "iou.update_state(test_labels_argmax, pred_argmax)\n",
        "print(\"Mean IoU =\", iou.result().numpy())"
      ],
      "metadata": {
        "id": "7GokLQdq-4Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate I0U for each class\n",
        "values = np.array(iou.get_weights()).reshape(3, 3) # number_of_classes, number_of_classes\n",
        "print(values)\n",
        "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[1,0]+ values[2,0])\n",
        "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[0,1]+ values[2,1])\n",
        "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[0,2]+ values[1,2])\n",
        "\n",
        "print(\"IoU for class1 is: \", class1_IoU)\n",
        "print(\"IoU for class2 is: \", class2_IoU)\n",
        "print(\"IoU for class3 is: \", class3_IoU)\n"
      ],
      "metadata": {
        "id": "Zslois29-5BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STAGE 3: SEGMENT NEW IMAGES\n",
        "\n",
        "# load pre-trained U-Net model from file\n",
        "model = tf.keras.models.load_model(os.path.join(wdir, '230918_model_batchsize7_LR-8e-4_70epochs.h5'))"
      ],
      "metadata": {
        "id": "kHJJpXawgNWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# define path to new images, and image size\n",
        "newdata_dir = '/content/drive/MyDrive/PhD/UV-BGF/230627_unet-test_individual-leaves/batch4_individualleaves_downsampled'\n",
        "newdata_paths = sorted([os.path.join(newdata_dir, fname)\n",
        "                        for fname in os.listdir(newdata_dir)\n",
        "                        if fname.endswith(\".png\")])\n",
        "img_size = (480, 480) # (height, width)\n",
        "\n",
        "# Load images and convert them to arrays.\n",
        "newdata_imgs = [cv2.imread(img, 1) for img in newdata_paths]\n",
        "newdata_imgs = np.array(newdata_imgs)\n",
        "\n",
        "# Normalize image data set so values are between 0 and 1\n",
        "newdata_imgs = newdata_imgs / 255\n",
        "\n",
        "# create new directories for mask and figure outputs to save to\n",
        "export_dir = '/content/drive/MyDrive/PhD/UV-BGF/230627_unet-test_individual-leaves/230918_model_batchsize7_LR-8e-4_70epochs_batch4'\n",
        "if not os.path.isdir(export_dir):\n",
        "        os.mkdir(export_dir)\n",
        "\n",
        "prediction_dir = os.path.join(export_dir, 'predictions')\n",
        "if not os.path.isdir(prediction_dir):\n",
        "        os.mkdir(prediction_dir)\n",
        "\n",
        "figure_dir = os.path.join(export_dir, 'figures')\n",
        "if not os.path.isdir(figure_dir):\n",
        "        os.mkdir(figure_dir)\n",
        "\n",
        "# apply model to all new images and save mask as image where 0 = background, 125 = uninfected leaf area, 250 = infected leaf area\n",
        "for i in list(range(0, len(newdata_imgs)-1)):\n",
        "    new_image = newdata_imgs[i]\n",
        "    prediction = model.predict(np.expand_dims(new_image, 0))\n",
        "    mask = np.argmax(prediction, axis=3)[0,:,:]\n",
        "    mask_img = mask * 125\n",
        "    mask_img = Image.fromarray(mask_img.astype(\"uint8\"))\n",
        "    mask_img.save(re.sub(newdata_dir, prediction_dir, newdata_paths[i]))\n",
        "\n",
        "    # optional: plot side-by-side input vs predicted mask\n",
        "    plt.close(1)\n",
        "    fig = plt.figure(1, figsize = (15,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(new_image[:,:,::-1], vmin = 0, vmax = 1, interpolation='nearest')\n",
        "    plt.title('Input Image')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(mask, vmin=0, vmax=2, cmap = 'viridis', interpolation='none')\n",
        "    plt.title('Predicted Mask')\n",
        "    fig.savefig(re.sub(newdata_dir, figure_dir, newdata_paths[i]), dpi = 150)\n"
      ],
      "metadata": {
        "id": "QUG1ptWrjhd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to calculate % UV-BGF from labels generated by CNN and the images used as input for the CNN\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "inputdir = prediction_dir\n",
        "\n",
        "# Create empty list to be filled with the label pixel counts and percent uv-bgf per image\n",
        "d=[]\n",
        "\n",
        "for filename in os.listdir(inputdir):\n",
        "        if '.png' in filename:\n",
        "            # Print out filename of currently processed image\n",
        "            print('Processing image: ' + filename)\n",
        "            labelimgpath = os.path.join(inputdir, filename)\n",
        "            labelimg = np.array(cv2.imread(labelimgpath, 0))\n",
        "            leafpixels = np.count_nonzero(labelimg == 125)\n",
        "            bgfpixels = np.count_nonzero(labelimg == 250)\n",
        "            percent_bgf = round(bgfpixels/(bgfpixels+leafpixels) * 100, 2)\n",
        "            #labelimg = np.array(Image.open(labelimgpath).convert('L'))\n",
        "            #labelpixels_uncorrected = np.count_nonzero(labelimg)\n",
        "            # Append current image filename and calculated parameters to list d\n",
        "            d.append(\n",
        "                {\n",
        "                    'imagefile': filename,\n",
        "                    'leaf_pixels': leafpixels,\n",
        "                    'bgf_pixels': bgfpixels,\n",
        "                    'percent_bgf': percent_bgf\n",
        "                    })\n",
        "# Convert list d to dataframe and save as CSV file in the input directory\n",
        "df = pd.DataFrame(d)\n",
        "outfile = os.path.join(inputdir, 'labelpixel_quantification.csv')\n",
        "df.to_csv(outfile, index=False)"
      ],
      "metadata": {
        "id": "j_MDoZNxqrZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}